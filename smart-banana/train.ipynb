{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc040f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cordana: 400 images\n",
      "healthy: 400 images\n",
      "pestalotiopsis: 400 images\n",
      "sigatoka: 400 images\n",
      "Found 1280 images belonging to 4 classes.\n",
      "Found 320 images belonging to 4 classes.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 160, 160, 3)]     0         \n",
      "                                                                 \n",
      " mobilenetv2_1.00_160 (Func  (None, 5, 5, 1280)        2257984   \n",
      " tional)                                                         \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 1280)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               163968    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2422468 (9.24 MB)\n",
      "Trainable params: 164484 (642.52 KB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# mobilenet_transfer_train.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ------- User settings -------\n",
    "base_dir = r'C:\\Users\\fab\\Documents\\projects\\smart-banana-expo\\smart-banana\\BananaLSD\\AugmentedSet'  # update if needed\n",
    "diseases = ['cordana', 'healthy', 'pestalotiopsis', 'sigatoka']\n",
    "img_height = 160   # smaller than 224 to save compute/size\n",
    "img_width = 160\n",
    "batch_size = 32\n",
    "epochs_stage1 = 8     # initial training with frozen base\n",
    "epochs_stage2 = 8     # fine-tuning after unfreeze\n",
    "learning_rate = 1e-4\n",
    "model_out_dir = 'saved_models'\n",
    "os.makedirs(model_out_dir, exist_ok=True)\n",
    "# -----------------------------\n",
    "\n",
    "# show counts\n",
    "for d in diseases:\n",
    "    p = os.path.join(base_dir, d)\n",
    "    n = len(os.listdir(p)) if os.path.exists(p) else 0\n",
    "    print(f\"{d}: {n} images\")\n",
    "\n",
    "# Data generators with preprocessing for MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = len(diseases)\n",
    "\n",
    "# Build MobileNetV2-based model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False,\n",
    "                         input_shape=(img_height, img_width, 3))\n",
    "base_model.trainable = False  # freeze the pretrained backbone\n",
    "\n",
    "inputs = layers.Input(shape=(img_height, img_width, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)   # small head\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d44e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.2574 - accuracy: 0.4820\n",
      "Epoch 1: val_accuracy improved from -inf to 0.77188, saving model to saved_models\\best_mobilenetv2_weights.h5\n",
      "40/40 [==============================] - 33s 780ms/step - loss: 1.2574 - accuracy: 0.4820 - val_loss: 0.7267 - val_accuracy: 0.7719 - lr: 1.0000e-04\n",
      "Epoch 2/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7686 - accuracy: 0.6961\n",
      "Epoch 2: val_accuracy improved from 0.77188 to 0.85312, saving model to saved_models\\best_mobilenetv2_weights.h5\n",
      "40/40 [==============================] - 15s 383ms/step - loss: 0.7686 - accuracy: 0.6961 - val_loss: 0.4630 - val_accuracy: 0.8531 - lr: 1.0000e-04\n",
      "Epoch 3/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5572 - accuracy: 0.7906\n",
      "Epoch 3: val_accuracy improved from 0.85312 to 0.87813, saving model to saved_models\\best_mobilenetv2_weights.h5\n",
      "40/40 [==============================] - 15s 380ms/step - loss: 0.5572 - accuracy: 0.7906 - val_loss: 0.3591 - val_accuracy: 0.8781 - lr: 1.0000e-04\n",
      "Epoch 4/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4597 - accuracy: 0.8195\n",
      "Epoch 4: val_accuracy improved from 0.87813 to 0.91562, saving model to saved_models\\best_mobilenetv2_weights.h5\n",
      "40/40 [==============================] - 16s 395ms/step - loss: 0.4597 - accuracy: 0.8195 - val_loss: 0.2749 - val_accuracy: 0.9156 - lr: 1.0000e-04\n",
      "Epoch 5/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3532 - accuracy: 0.8742\n",
      "Epoch 5: val_accuracy improved from 0.91562 to 0.94375, saving model to saved_models\\best_mobilenetv2_weights.h5\n",
      "40/40 [==============================] - 15s 379ms/step - loss: 0.3532 - accuracy: 0.8742 - val_loss: 0.2404 - val_accuracy: 0.9438 - lr: 1.0000e-04\n",
      "Epoch 6/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.8938\n",
      "Epoch 6: val_accuracy did not improve from 0.94375\n",
      "40/40 [==============================] - 15s 378ms/step - loss: 0.3094 - accuracy: 0.8938 - val_loss: 0.2185 - val_accuracy: 0.9281 - lr: 1.0000e-04\n",
      "Epoch 7/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3091 - accuracy: 0.8820\n",
      "Epoch 7: val_accuracy did not improve from 0.94375\n",
      "40/40 [==============================] - 15s 377ms/step - loss: 0.3091 - accuracy: 0.8820 - val_loss: 0.2359 - val_accuracy: 0.9219 - lr: 1.0000e-04\n",
      "Epoch 8/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.9047\n",
      "Epoch 8: val_accuracy did not improve from 0.94375\n",
      "40/40 [==============================] - 15s 373ms/step - loss: 0.2844 - accuracy: 0.9047 - val_loss: 0.2024 - val_accuracy: 0.9375 - lr: 1.0000e-04\n",
      "Epoch 1/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3022 - accuracy: 0.8984\n",
      "Epoch 1: val_accuracy did not improve from 0.94375\n",
      "40/40 [==============================] - 24s 516ms/step - loss: 0.3022 - accuracy: 0.8984 - val_loss: 0.1873 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Epoch 2/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.9102\n",
      "Epoch 2: val_accuracy did not improve from 0.94375\n",
      "40/40 [==============================] - 21s 523ms/step - loss: 0.2365 - accuracy: 0.9102 - val_loss: 0.1867 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Epoch 3/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2080 - accuracy: 0.9266\n",
      "Epoch 3: val_accuracy improved from 0.94375 to 0.95625, saving model to saved_models\\best_mobilenetv2_weights.h5\n",
      "40/40 [==============================] - 20s 495ms/step - loss: 0.2080 - accuracy: 0.9266 - val_loss: 0.1453 - val_accuracy: 0.9563 - lr: 1.0000e-05\n",
      "Epoch 4/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1494 - accuracy: 0.9484\n",
      "Epoch 4: val_accuracy did not improve from 0.95625\n",
      "40/40 [==============================] - 20s 496ms/step - loss: 0.1494 - accuracy: 0.9484 - val_loss: 0.1308 - val_accuracy: 0.9531 - lr: 1.0000e-05\n",
      "Epoch 5/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9555\n",
      "Epoch 5: val_accuracy improved from 0.95625 to 0.97188, saving model to saved_models\\best_mobilenetv2_weights.h5\n",
      "40/40 [==============================] - 20s 497ms/step - loss: 0.1300 - accuracy: 0.9555 - val_loss: 0.0967 - val_accuracy: 0.9719 - lr: 1.0000e-05\n",
      "Epoch 6/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1270 - accuracy: 0.9539\n",
      "Epoch 6: val_accuracy did not improve from 0.97188\n",
      "40/40 [==============================] - 20s 494ms/step - loss: 0.1270 - accuracy: 0.9539 - val_loss: 0.0918 - val_accuracy: 0.9719 - lr: 1.0000e-05\n",
      "Epoch 7/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9664\n",
      "Epoch 7: val_accuracy did not improve from 0.97188\n",
      "40/40 [==============================] - 20s 497ms/step - loss: 0.0962 - accuracy: 0.9664 - val_loss: 0.0900 - val_accuracy: 0.9688 - lr: 1.0000e-05\n",
      "Epoch 8/8\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9625\n",
      "Epoch 8: val_accuracy did not improve from 0.97188\n",
      "40/40 [==============================] - 21s 534ms/step - loss: 0.1116 - accuracy: 0.9625 - val_loss: 0.1141 - val_accuracy: 0.9656 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Save only weights\n",
    "checkpoint_path = os.path.join(model_out_dir, 'best_mobilenetv2_weights.h5')\n",
    "callbacks = [\n",
    "    ModelCheckpoint(checkpoint_path,\n",
    "                    monitor='val_accuracy',\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=True,   # <-- important\n",
    "                    verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Stage 1: train head (backbone frozen)\n",
    "history1 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=epochs_stage1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model.load_weights(checkpoint_path)   # load weights into same architecture\n",
    "\n",
    "\n",
    "# Stage 2: fine-tune - unfreeze last few layers of backbone\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except last N blocks to avoid big changes\n",
    "# You can experiment with unfreezing more/less\n",
    "fine_tune_at = 100  # layer index to start fine-tuning (tweak if needed)\n",
    "\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    layer.trainable = i >= fine_tune_at\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate/10),  # lower LR for fine-tuning\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=epochs_stage2,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c2b9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_keras_path = os.path.join(model_out_dir, 'banana_mobilenetv2_final.keras')\n",
    "model.save(final_keras_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51468710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 6s 593ms/step - loss: 0.0937 - accuracy: 0.9625\n",
      "Final validation accuracy: 96.25%\n",
      "Keras model saved to: saved_models\\banana_mobilenetv2_final.keras\n",
      "Saved class indices to: saved_models\\class_indices.json\n",
      "Keras model size: 25.26 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Final evaluation\n",
    "loss, acc = model.evaluate(validation_generator)\n",
    "print(f\"Final validation accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# Save final Keras model (SavedModel format or .keras)\n",
    "final_keras_path = os.path.join(model_out_dir, 'banana_mobilenetv2_final.keras')\n",
    "model.save(final_keras_path)\n",
    "print(f\"Keras model saved to: {final_keras_path}\")\n",
    "\n",
    "# Save class indices for inference later\n",
    "import json\n",
    "class_indices_path = os.path.join(model_out_dir, 'class_indices.json')\n",
    "with open(class_indices_path, 'w') as f:\n",
    "    json.dump(train_generator.class_indices, f)\n",
    "print(f\"Saved class indices to: {class_indices_path}\")\n",
    "\n",
    "# Show file size\n",
    "def sizeof(path):\n",
    "    size = os.path.getsize(path) / (1024*1024)\n",
    "    return f\"{size:.2f} MB\"\n",
    "\n",
    "print(\"Keras model size:\", sizeof(final_keras_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb5faed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\fab\\AppData\\Local\\Temp\\tmpnqmvwqn4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\fab\\AppData\\Local\\Temp\\tmpnqmvwqn4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite (dynamic quant) to: saved_models\\banana_mobilenetv2_dynamic_quant.tflite\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sizeof' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(tflite_model)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved TFLite (dynamic quant) to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tflite_path)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTFLite size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43msizeof\u001b[49m(tflite_path))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# (2) Full integer quantization (requires a calibration dataset generator)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresentative_data_gen\u001b[39m():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sizeof' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Convert to TFLite with post-training quantization (smaller file)\n",
    "# -----------------------------\n",
    "# (1) Basic dynamic range quantization\n",
    "# Load the Keras model file\n",
    "model = tf.keras.models.load_model(final_keras_path)\n",
    "\n",
    "# Convert directly from the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "tflite_path = os.path.join(model_out_dir, 'banana_mobilenetv2_dynamic_quant.tflite')\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(\"Saved TFLite (dynamic quant) to:\", tflite_path)\n",
    "print(\"TFLite size:\", sizeof(tflite_path))\n",
    "\n",
    "# (2) Full integer quantization (requires a calibration dataset generator)\n",
    "def representative_data_gen():\n",
    "    for i in range(100):  # 100 calibration samples\n",
    "        img, _ = next(train_generator)\n",
    "        yield [img.astype(np.float32)]\n",
    "\n",
    "# Load the Keras model file\n",
    "model = tf.keras.models.load_model(final_keras_path)\n",
    "\n",
    "# Convert directly from the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# to ensure integer ops:\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8  # or tf.int8\n",
    "converter.inference_output_type = tf.uint8\n",
    "try:\n",
    "    tflite_int8 = converter.convert()\n",
    "    tflite_int8_path = os.path.join(model_out_dir, 'banana_mobilenetv2_int8.tflite')\n",
    "    with open(tflite_int8_path, 'wb') as f:\n",
    "        f.write(tflite_int8)\n",
    "    print(\"Saved TFLite (int8) to:\", tflite_int8_path)\n",
    "    print(\"TFLite int8 size:\", sizeof(tflite_int8_path))\n",
    "except Exception as e:\n",
    "    print(\"Integer quantization failed (may not be supported on all ops/devices):\", e)\n",
    "\n",
    "# (optional) Float16 quantization (good tradeoff)\n",
    "# Load the Keras model file\n",
    "model = tf.keras.models.load_model(final_keras_path)\n",
    "\n",
    "# Convert directly from the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_fp16 = converter.convert()\n",
    "tflite_fp16_path = os.path.join(model_out_dir, 'banana_mobilenetv2_fp16.tflite')\n",
    "with open(tflite_fp16_path, 'wb') as f:\n",
    "    f.write(tflite_fp16)\n",
    "print(\"Saved TFLite (float16) to:\", tflite_fp16_path)\n",
    "print(\"TFLite fp16 size:\", sizeof(tflite_fp16_path))\n",
    "\n",
    "# Quick test: run one prediction with TFLite dynamic quant model\n",
    "import numpy as np\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# read one sample from validation set\n",
    "x_val, y_val = next(validation_generator)\n",
    "sample = x_val[0:1]  # single image\n",
    "# If model expects float input (dynamic quant uses float), set directly\n",
    "interpreter.set_tensor(input_details[0]['index'], sample.astype(np.float32))\n",
    "interpreter.invoke()\n",
    "pred = interpreter.get_tensor(output_details[0]['index'])\n",
    "pred_class = np.argmax(pred, axis=-1)[0]\n",
    "print(\"TFLite predicted class index:\", pred_class)\n",
    "print(\"Mapping:\", train_generator.class_indices)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
