{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc040f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cordana: 400 images\n",
      "healthy: 400 images\n",
      "pestalotiopsis: 400 images\n",
      "sigatoka: 400 images\n",
      "Found 1280 images belonging to 4 classes.\n",
      "Found 320 images belonging to 4 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_160            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_160            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,468</span> (9.24 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,422,468\u001b[0m (9.24 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,484</span> (642.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,484\u001b[0m (642.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mobilenet_transfer_train.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ------- User settings -------\n",
    "base_dir = r'C:\\Users\\fab\\Documents\\projects\\smart-banana-expo\\smart-banana\\BananaLSD\\AugmentedSet'  # update if needed\n",
    "diseases = ['cordana', 'healthy', 'pestalotiopsis', 'sigatoka']\n",
    "img_height = 160   # smaller than 224 to save compute/size\n",
    "img_width = 160\n",
    "batch_size = 32\n",
    "epochs_stage1 = 8     # initial training with frozen base\n",
    "epochs_stage2 = 8     # fine-tuning after unfreeze\n",
    "learning_rate = 1e-4\n",
    "model_out_dir = 'saved_models'\n",
    "os.makedirs(model_out_dir, exist_ok=True)\n",
    "# -----------------------------\n",
    "\n",
    "# show counts\n",
    "for d in diseases:\n",
    "    p = os.path.join(base_dir, d)\n",
    "    n = len(os.listdir(p)) if os.path.exists(p) else 0\n",
    "    print(f\"{d}: {n} images\")\n",
    "\n",
    "# Data generators with preprocessing for MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = len(diseases)\n",
    "\n",
    "# Build MobileNetV2-based model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False,\n",
    "                         input_shape=(img_height, img_width, 3))\n",
    "base_model.trainable = False  # freeze the pretrained backbone\n",
    "\n",
    "inputs = layers.Input(shape=(img_height, img_width, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)   # small head\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d44e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3400 - loss: 1.7243\n",
      "Epoch 1: val_accuracy improved from None to 0.64062, saving model to saved_models\\best_mobilenetv2.weights.h5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 2s/step - accuracy: 0.4141 - loss: 1.4745 - val_accuracy: 0.6406 - val_loss: 0.8494 - learning_rate: 1.0000e-04\n",
      "Epoch 2/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591ms/step - accuracy: 0.6075 - loss: 0.9621\n",
      "Epoch 2: val_accuracy improved from 0.64062 to 0.80937, saving model to saved_models\\best_mobilenetv2.weights.h5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 768ms/step - accuracy: 0.6578 - loss: 0.8556 - val_accuracy: 0.8094 - val_loss: 0.5213 - learning_rate: 1.0000e-04\n",
      "Epoch 3/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629ms/step - accuracy: 0.7216 - loss: 0.6731\n",
      "Epoch 3: val_accuracy improved from 0.80937 to 0.86250, saving model to saved_models\\best_mobilenetv2.weights.h5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 851ms/step - accuracy: 0.7453 - loss: 0.6317 - val_accuracy: 0.8625 - val_loss: 0.4165 - learning_rate: 1.0000e-04\n",
      "Epoch 4/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712ms/step - accuracy: 0.7953 - loss: 0.5370\n",
      "Epoch 4: val_accuracy did not improve from 0.86250\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 886ms/step - accuracy: 0.8102 - loss: 0.5010 - val_accuracy: 0.8594 - val_loss: 0.3519 - learning_rate: 1.0000e-04\n",
      "Epoch 5/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660ms/step - accuracy: 0.8589 - loss: 0.3967\n",
      "Epoch 5: val_accuracy improved from 0.86250 to 0.89688, saving model to saved_models\\best_mobilenetv2.weights.h5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 860ms/step - accuracy: 0.8414 - loss: 0.4236 - val_accuracy: 0.8969 - val_loss: 0.3156 - learning_rate: 1.0000e-04\n",
      "Epoch 6/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652ms/step - accuracy: 0.8301 - loss: 0.4152\n",
      "Epoch 6: val_accuracy improved from 0.89688 to 0.91562, saving model to saved_models\\best_mobilenetv2.weights.h5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 850ms/step - accuracy: 0.8539 - loss: 0.3926 - val_accuracy: 0.9156 - val_loss: 0.2666 - learning_rate: 1.0000e-04\n",
      "Epoch 7/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655ms/step - accuracy: 0.9070 - loss: 0.3113\n",
      "Epoch 7: val_accuracy did not improve from 0.91562\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 841ms/step - accuracy: 0.9000 - loss: 0.3130 - val_accuracy: 0.8969 - val_loss: 0.2745 - learning_rate: 1.0000e-04\n",
      "Epoch 8/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737ms/step - accuracy: 0.8623 - loss: 0.3596\n",
      "Epoch 8: val_accuracy improved from 0.91562 to 0.91875, saving model to saved_models\\best_mobilenetv2.weights.h5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 934ms/step - accuracy: 0.8836 - loss: 0.3282 - val_accuracy: 0.9187 - val_loss: 0.2350 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Epoch 1/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887ms/step - accuracy: 0.6927 - loss: 0.7687\n",
      "Epoch 1: val_accuracy improved from 0.91875 to 0.94063, saving model to saved_models\\best_mobilenetv2.weights.h5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 1s/step - accuracy: 0.7297 - loss: 0.6856 - val_accuracy: 0.9406 - val_loss: 0.2005 - learning_rate: 1.0000e-05\n",
      "Epoch 2/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872ms/step - accuracy: 0.7941 - loss: 0.5523\n",
      "Epoch 2: val_accuracy did not improve from 0.94063\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.8070 - loss: 0.5333 - val_accuracy: 0.9281 - val_loss: 0.2283 - learning_rate: 1.0000e-05\n",
      "Epoch 3/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972ms/step - accuracy: 0.8285 - loss: 0.4848\n",
      "Epoch 3: val_accuracy improved from 0.94063 to 0.95000, saving model to saved_models\\best_mobilenetv2.weights.h5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 1s/step - accuracy: 0.8461 - loss: 0.4380 - val_accuracy: 0.9500 - val_loss: 0.2128 - learning_rate: 1.0000e-05\n",
      "Epoch 4/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8817 - loss: 0.3586\n",
      "Epoch 4: val_accuracy did not improve from 0.95000\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - accuracy: 0.8750 - loss: 0.3665 - val_accuracy: 0.9125 - val_loss: 0.2464 - learning_rate: 1.0000e-05\n",
      "Epoch 5/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985ms/step - accuracy: 0.8733 - loss: 0.3496\n",
      "Epoch 5: val_accuracy did not improve from 0.95000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - accuracy: 0.8828 - loss: 0.3438 - val_accuracy: 0.9312 - val_loss: 0.2146 - learning_rate: 2.0000e-06\n",
      "Epoch 6/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890ms/step - accuracy: 0.8862 - loss: 0.3434\n",
      "Epoch 6: val_accuracy did not improve from 0.95000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.8844 - loss: 0.3377 - val_accuracy: 0.9219 - val_loss: 0.2448 - learning_rate: 2.0000e-06\n",
      "Epoch 7/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902ms/step - accuracy: 0.9002 - loss: 0.3178\n",
      "Epoch 7: val_accuracy did not improve from 0.95000\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.8930 - loss: 0.3320 - val_accuracy: 0.9187 - val_loss: 0.2638 - learning_rate: 2.0000e-06\n",
      "Epoch 8/8\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879ms/step - accuracy: 0.9039 - loss: 0.3063\n",
      "Epoch 8: val_accuracy did not improve from 0.95000\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9008 - loss: 0.3169 - val_accuracy: 0.9375 - val_loss: 0.1941 - learning_rate: 1.0000e-06\n",
      "Restoring model weights from the end of the best epoch: 3.\n"
     ]
    }
   ],
   "source": [
    "# Save only weights\n",
    "checkpoint_path = os.path.join(model_out_dir, 'best_mobilenetv2.weights.h5')  # <- fix here\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy', patience=8, restore_best_weights=True, verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "# Stage 1: train head (backbone frozen)\n",
    "history1 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=epochs_stage1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model.load_weights(checkpoint_path)   # load weights into same architecture\n",
    "\n",
    "\n",
    "# Stage 2: fine-tune - unfreeze last few layers of backbone\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except last N blocks to avoid big changes\n",
    "# You can experiment with unfreezing more/less\n",
    "fine_tune_at = 100  # layer index to start fine-tuning (tweak if needed)\n",
    "\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    layer.trainable = i >= fine_tune_at\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate/10),  # lower LR for fine-tuning\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=epochs_stage2,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c2b9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_keras_path = os.path.join(model_out_dir, 'banana_mobilenetv2_final.keras')\n",
    "model.save(final_keras_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51468710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 318ms/step - accuracy: 0.9062 - loss: 0.2420\n",
      "Final validation accuracy: 90.62%\n",
      "Keras model saved to: saved_models\\banana_mobilenetv2_final.keras\n",
      "Saved class indices to: saved_models\\class_indices.json\n",
      "Keras model size: 25.30 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Final evaluation\n",
    "loss, acc = model.evaluate(validation_generator)\n",
    "print(f\"Final validation accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# Save final Keras model (SavedModel format or .keras)\n",
    "final_keras_path = os.path.join(model_out_dir, 'banana_mobilenetv2_final.keras')\n",
    "model.save(final_keras_path)\n",
    "print(f\"Keras model saved to: {final_keras_path}\")\n",
    "\n",
    "# Save class indices for inference later\n",
    "import json\n",
    "class_indices_path = os.path.join(model_out_dir, 'class_indices.json')\n",
    "with open(class_indices_path, 'w') as f:\n",
    "    json.dump(train_generator.class_indices, f)\n",
    "print(f\"Saved class indices to: {class_indices_path}\")\n",
    "\n",
    "# Show file size\n",
    "def sizeof(path):\n",
    "    size = os.path.getsize(path) / (1024*1024)\n",
    "    return f\"{size:.2f} MB\"\n",
    "\n",
    "print(\"Keras model size:\", sizeof(final_keras_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb5faed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\fab\\AppData\\Local\\Temp\\tmpnqmvwqn4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\fab\\AppData\\Local\\Temp\\tmpnqmvwqn4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite (dynamic quant) to: saved_models\\banana_mobilenetv2_dynamic_quant.tflite\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sizeof' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(tflite_model)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved TFLite (dynamic quant) to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tflite_path)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTFLite size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43msizeof\u001b[49m(tflite_path))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# (2) Full integer quantization (requires a calibration dataset generator)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresentative_data_gen\u001b[39m():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sizeof' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Convert to TFLite with post-training quantization (smaller file)\n",
    "# -----------------------------\n",
    "# (1) Basic dynamic range quantization\n",
    "# Load the Keras model file\n",
    "model = tf.keras.models.load_model(final_keras_path)\n",
    "\n",
    "# Convert directly from the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "tflite_path = os.path.join(model_out_dir, 'banana_mobilenetv2_dynamic_quant.tflite')\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(\"Saved TFLite (dynamic quant) to:\", tflite_path)\n",
    "print(\"TFLite size:\", sizeof(tflite_path))\n",
    "\n",
    "# (2) Full integer quantization (requires a calibration dataset generator)\n",
    "def representative_data_gen():\n",
    "    for i in range(100):  # 100 calibration samples\n",
    "        img, _ = next(train_generator)\n",
    "        yield [img.astype(np.float32)]\n",
    "\n",
    "# Load the Keras model file\n",
    "model = tf.keras.models.load_model(final_keras_path)\n",
    "\n",
    "# Convert directly from the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# to ensure integer ops:\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8  # or tf.int8\n",
    "converter.inference_output_type = tf.uint8\n",
    "try:\n",
    "    tflite_int8 = converter.convert()\n",
    "    tflite_int8_path = os.path.join(model_out_dir, 'banana_mobilenetv2_int8.tflite')\n",
    "    with open(tflite_int8_path, 'wb') as f:\n",
    "        f.write(tflite_int8)\n",
    "    print(\"Saved TFLite (int8) to:\", tflite_int8_path)\n",
    "    print(\"TFLite int8 size:\", sizeof(tflite_int8_path))\n",
    "except Exception as e:\n",
    "    print(\"Integer quantization failed (may not be supported on all ops/devices):\", e)\n",
    "\n",
    "# (optional) Float16 quantization (good tradeoff)\n",
    "# Load the Keras model file\n",
    "model = tf.keras.models.load_model(final_keras_path)\n",
    "\n",
    "# Convert directly from the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_fp16 = converter.convert()\n",
    "tflite_fp16_path = os.path.join(model_out_dir, 'banana_mobilenetv2_fp16.tflite')\n",
    "with open(tflite_fp16_path, 'wb') as f:\n",
    "    f.write(tflite_fp16)\n",
    "print(\"Saved TFLite (float16) to:\", tflite_fp16_path)\n",
    "print(\"TFLite fp16 size:\", sizeof(tflite_fp16_path))\n",
    "\n",
    "# Quick test: run one prediction with TFLite dynamic quant model\n",
    "import numpy as np\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# read one sample from validation set\n",
    "x_val, y_val = next(validation_generator)\n",
    "sample = x_val[0:1]  # single image\n",
    "# If model expects float input (dynamic quant uses float), set directly\n",
    "interpreter.set_tensor(input_details[0]['index'], sample.astype(np.float32))\n",
    "interpreter.invoke()\n",
    "pred = interpreter.get_tensor(output_details[0]['index'])\n",
    "pred_class = np.argmax(pred, axis=-1)[0]\n",
    "print(\"TFLite predicted class index:\", pred_class)\n",
    "print(\"Mapping:\", train_generator.class_indices)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
